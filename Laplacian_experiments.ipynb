{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laplacian_experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNypKlo+uEw1uGYo6fBYmTR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBZE7GQudj5F"
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "\n",
        "def grad(y, x):\n",
        "    g = autograd.grad(y, [x], grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
        "    return g\n",
        "\n",
        "def div(y, x):\n",
        "    div = 0.0\n",
        "    for i in range(y.shape[-1]):\n",
        "        div += autograd.grad(y[..., i], x, grad_outputs=torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "    return div\n",
        "\n",
        "def Laplacian(y, x):\n",
        "    g = grad(y, x)\n",
        "    return div(g, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdriGsgFd14Q"
      },
      "source": [
        "def f(x):\n",
        "  return x[0,0]**2 + x[0,1]**2 - 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4le6y5de1E6",
        "outputId": "a98dad46-a2aa-457b-f2f6-aa43516aa740"
      },
      "source": [
        "x = torch.tensor([[2.0,2.0]], requires_grad=True)\n",
        "y = f(x)\n",
        "z = Laplacian(y, x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7., grad_fn=<SubBackward0>)\n",
            "tensor([[4.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}